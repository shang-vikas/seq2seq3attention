
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/attention_module.ipynb

import os,sys,time,copy,math,numpy as np
from IPython.core.debugger import set_trace
import torch,torch.nn as nn,torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns

class EncoderDecoder(nn.Module):
    '''
    A standard encoder-decoder architecture...
    '''
    def __init__(self,encr,decr,src_emb,tgt_emb,gen):
        super().__init__()
        self.encr = encr;self.decr = decr; self.src_emb = src_emb
        self.tgt_emb = tgt_emb; self.gen = gen

    def forward(self,src,tgt,src_mask,tgt_mask):
        return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask)

    def encode(self,src,src_mask):
        return self.encr(self.src_emb(src),src_mask)

    def decode(self,tgt,tgt_mask):
        return self.decr(self.tgt_emb(tgt),tgt_mask)


class Generator(nn.Module):
    """
    Defines standard linear+ softmax step
    """
    def __init__(self,d_model,vocab):
        super().__init__()
        self.proj = nn.Linear(d_model,vocab)

    def forward(self,x):
        return F.log_softmax(self.proj(x),dim=-1)

def clones(module,N):
    ''' Produces N identical modules '''
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

class Encoder(nn.Module):
    ''' Core encoder is a stack of N layers '''
    def __init__(self,layer,N):
        super().__init__()
        self.layers = clones(layer,N)
        self.norm  = LayerNorm(layer.size)

    def forward(self,x,mask):
        ''' Pass the input and mask through each layer in turn  '''
        for layer in self.layers:
            x = layer(x,mask)
        return self.norm(x)


class LayerNorm(nn.Module):
    def __init__(self,features,eps=1e-6):
        super().__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self,x):
        mean = x.mean(-1,keepdim=True)
        std = x.std(-1,keepdim=True)
        return self.a_2*(x - mean)/(std + self.eps) + self.b_2

class SubLayerConnection(nn.Module):
    """ A sum of original + layer(input) followed by layer Norm"""
    def __init__(self,size,p):
        super().__init__()
        self.norm = LayerNorm(size)
        self.p = nn.Dropout(p)

    def forward(self,x,sublayer):
        """ Apply residual connection to sublayer of same size"""
        return x + self.p(sublayer(self.norm(x)))

class BasicEncoderLayer(nn.Module):
    """ Basic encoder layer is made of self.attentiona and feed forward layer """
    def __init__(self,sz,slf_att,ff,p):
        super().__init__()
        self.slf_att = slf_att; self.sz = sz; self.ff = ff;
        self.sublayer = clones(SubLayerConnection(size,dropout),2)

    def forward(self,x):
        x = self.sublayer[0](x,lambda x: self.slf_att(x,x,x,mask))
        return self.sublayer[1](x,self.ff)

class Decoder(nn.Module):
    ''' Basic N layer Decoder '''
    def __init__(self,layer,N):
        super().__init__()
        self.layers = clones(layer,N)
        self.norm = LayerNorm(layer.size)

    def forward(self,x,mem,src_msk,tgt_msk):
        for layer in self.layers:
            x = layer(x,mem,src_msk,tgt_msk)
        return self.norm(x)

class BasicDecoderLayer(nn.Module):
    ''' Basic N layer Decoder '''
    def __init__(self,sz,slf_att,src_att,ff,p):
        super().__init__()
        self.slf_att = slf_att; self.src_att = src_att; self.ff = ff;
        self.sublayer  = clones(SubLayerConnection(sz,p), 3)

    def forward(self,x,mem,src_msk,tgt_msk):
        x = self.sublayer[0](x, lambda x: self.slf_att(x,x,x,tgt_msk))
        x = self.sublayer[1](x , lambda x: self.src_att(x,x,x,src_msk))
        return self.sublayer[2](x, lambda x:self.ff(x))

def future_msk(sz):
    """ Mask out future positions. """
    att_shp = (1,sz,sz)
    future_msk = np.triu(np.ones(att_shp),k=1).astype('uint8')
    return torch.from_numpy(future_msk) ==0

def attention(q,k,v,msk=None,p=None):
    """ Compute 'Scaled dot product attention' """
    d_k = q.size(-1)
    scores = torch.matmul(q,k.transpose(-2,-1))/math.sqrt(d_k)
    if msk is not None:
        scores = scores.masked_fill(mask==0,-1e9)
        p_att = F.softmax(scores,dim = -1)
        if p is not None:
            p_att = p(p_att)
        return torch.matmul(p_att,v), p_att