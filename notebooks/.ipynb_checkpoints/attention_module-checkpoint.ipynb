{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os,sys,time,copy,math,numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import torch,torch.nn as nn,torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class EncoderDecoder(nn.Module):\n",
    "    '''\n",
    "    A standard encoder-decoder architecture...\n",
    "    '''\n",
    "    def __init__(self,encr,decr,src_emb,tgt_emb,gen):\n",
    "        super().__init__()\n",
    "        self.encr = encr;self.decr = decr; self.src_emb = src_emb\n",
    "        self.tgt_emb = tgt_emb; self.gen = gen\n",
    "    \n",
    "    def forward(self,src,tgt,src_mask,tgt_mask):\n",
    "        return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask)\n",
    "    \n",
    "    def encode(self,src,src_mask):\n",
    "        return self.encr(self.src_emb(src),src_mask)\n",
    "    \n",
    "    def decode(self,tgt,tgt_mask):\n",
    "        return self.decr(self.tgt_emb(tgt),tgt_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines standard linear+ softmax step\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,vocab):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model,vocab)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return F.log_softmax(self.proj(x),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def clones(module,N):\n",
    "    ''' Produces N identical modules '''\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Encoder(nn.Module):\n",
    "    ''' Core encoder is a stack of N layers '''\n",
    "    def __init__(self,layer,N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer,N)\n",
    "        self.norm  = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        ''' Pass the input and mask through each layer in turn  '''\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return self.norm(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))        \n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(-1,keepdim=True)\n",
    "        std = x.std(-1,keepdim=True)\n",
    "        return self.a_2*(x - mean)/(std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SubLayerConnection(nn.Module):\n",
    "    \"\"\" A sum of original + layer(input) followed by layer Norm\"\"\"\n",
    "    def __init__(self,size,p):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.p = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self,x,sublayer):\n",
    "        \"\"\" Apply residual connection to sublayer of same size\"\"\"\n",
    "        return x + self.p(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BasicEncoderLayer(nn.Module):\n",
    "    \"\"\" Basic encoder layer is made of self.attentiona and feed forward layer \"\"\"\n",
    "    def __init__(self,sz,slf_att,ff,p):\n",
    "        super().__init__()\n",
    "        self.slf_att = slf_att; self.sz = sz; self.ff = ff;\n",
    "        self.sublayer = clones(SubLayerConnection(size,dropout),2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.sublayer[0](x,lambda x: self.slf_att(x,x,x,mask))\n",
    "        return self.sublayer[1](x,self.ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Decoder(nn.Module):\n",
    "    ''' Basic N layer Decoder '''\n",
    "    def __init__(self,layer,N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer,N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self,x,mem,src_msk,tgt_msk):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mem,src_msk,tgt_msk)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BasicDecoderLayer(nn.Module):\n",
    "    ''' Basic N layer Decoder '''\n",
    "    def __init__(self,sz,slf_att,src_att,ff,p):\n",
    "        super().__init__()\n",
    "        self.slf_att = slf_att; self.src_att = src_att; self.ff = ff;\n",
    "        self.sublayer  = clones(SubLayerConnection(sz,p), 3)\n",
    "        \n",
    "    def forward(self,x,mem,src_msk,tgt_msk):\n",
    "        x = self.sublayer[0](x, lambda x: self.slf_att(x,x,x,tgt_msk))\n",
    "        x = self.sublayer[1](x , lambda x: self.src_att(x,x,x,src_msk))\n",
    "        return self.sublayer[2](x, lambda x:self.ff(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def future_msk(sz):\n",
    "    \"\"\" Mask out future positions. \"\"\"\n",
    "    att_shp = (1,sz,sz)\n",
    "    future_msk = np.triu(np.ones(att_shp),k=1).astype('uint8')\n",
    "    return torch.from_numpy(future_msk) ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb3390b4a90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(future_msk(4)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def attention(q,k,v,msk=None,p=None):\n",
    "    \"\"\" Compute 'Scaled dot product attention' \"\"\"\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q,k.transpose(-2,-1))/math.sqrt(d_k)\n",
    "    if msk is not None:\n",
    "        scores = scores.masked_fill(mask==0,-1e9)\n",
    "        p_att = F.softmax(scores,dim = -1)\n",
    "        if p is not None:\n",
    "            p_att = p(p_att)\n",
    "        return torch.matmul(p_att,v), p_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted attention_module.ipynb to nb_attention.py\r\n"
     ]
    }
   ],
   "source": [
    "! ../notebook2script.py ./attention_module.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyfast]",
   "language": "python",
   "name": "conda-env-pyfast-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
