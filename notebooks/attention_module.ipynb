{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os,sys,time,copy,math,numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import torch,torch.nn as nn,torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class EncoderDecoder(nn.Module):\n",
    "    '''\n",
    "    A standard encoder-decoder architecture...\n",
    "    '''\n",
    "    def __init__(self,encr,decr,src_emb,tgt_emb,gen):\n",
    "        super().__init__()\n",
    "        self.encr = encr;self.decr = decr; self.src_emb = src_emb\n",
    "        self.tgt_emb = tgt_emb; self.gen = gen\n",
    "    \n",
    "    def forward(self,src,tgt,src_mask,tgt_mask):\n",
    "        return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask)\n",
    "    \n",
    "    def encode(self,src,src_mask):\n",
    "        return self.encr(self.src_emb(src),src_mask)\n",
    "    \n",
    "    def decode(self,tgt,tgt_mask):\n",
    "        return self.decr(self.tgt_emb(tgt),tgt_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines standard linear+ softmax step\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,vocab):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model,vocab)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return F.log_softmax(self.proj(x),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def clones(module,N):\n",
    "    ''' Produces N identical modules '''\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Encoder(nn.Module):\n",
    "    ''' Core encoder is a stack of N layers '''\n",
    "    def __init__(self,layer,N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer,N)\n",
    "        self.norm  = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        ''' Pass the input and mask through each layer in turn  '''\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return self.norm(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))        \n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(-1,keepdim=True)\n",
    "        std = x.std(-1,keepdim=True)\n",
    "        return self.a_2*(x - mean)/(std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SubLayerConnection(nn.Module):\n",
    "    \"\"\" A sum of original + layer(input) followed by layer Norm\"\"\"\n",
    "    def __init__(self,size,p):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.p = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self,x,sublayer):\n",
    "        \"\"\" Apply residual connection to sublayer of same size\"\"\"\n",
    "        return x + self.p(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BasicEncoderLayer(nn.Module):\n",
    "    \"\"\" Basic encoder layer is made of self.attentiona and feed forward layer \"\"\"\n",
    "    def __init__(self,sz,slf_att,ff,p):\n",
    "        super().__init__()\n",
    "        self.slf_att = slf_att; self.sz = sz; self.ff = ff;\n",
    "        self.sublayer = clones(SubLayerConnection(size,dropout),2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.sublayer[0](x,lambda x: self.slf_att(x,x,x,mask))\n",
    "        return self.sublayer[1](x,self.ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Decoder(nn.Module):\n",
    "    ''' Basic N layer Decoder '''\n",
    "    def __init__(self,layer,N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer,N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self,x,mem,src_msk,tgt_msk):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mem,src_msk,tgt_msk)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BasicDecoderLayer(nn.Module):\n",
    "    ''' Basic N layer Decoder '''\n",
    "    def __init__(self,sz,slf_att,src_att,ff,p):\n",
    "        super().__init__()\n",
    "        self.slf_att = slf_att; self.src_att = src_att; self.ff = ff;\n",
    "        self.sublayer  = clones(SubLayerConnection(sz,p), 3)\n",
    "        \n",
    "    def forward(self,x,mem,src_msk,tgt_msk):\n",
    "        x = self.sublayer[0](x, lambda x: self.slf_att(x,x,x,tgt_msk))\n",
    "        x = self.sublayer[1](x , lambda x: self.src_att(x,x,x,src_msk))\n",
    "        return self.sublayer[2](x, lambda x:self.ff(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def future_msk(sz):\n",
    "    \"\"\" Mask out future positions. \"\"\"\n",
    "    att_shp = (1,sz,sz)\n",
    "    future_msk = np.triu(np.ones(att_shp),k=1).astype('uint8')\n",
    "    return torch.from_numpy(future_msk) ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb3390b4a90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(future_msk(4)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def attention(q,k,v,msk=None,p=None):\n",
    "    \"\"\" Compute 'Scaled dot product attention' \"\"\"\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q,k.transpose(-2,-1))/math.sqrt(d_k)\n",
    "    if msk is not None:\n",
    "        scores = scores.masked_fill(mask==0,-1e9)\n",
    "        p_att = F.softmax(scores,dim = -1)\n",
    "        if p is not None:\n",
    "            p_att = p(p_att)\n",
    "        return torch.matmul(p_att,v), p_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,h, d_model, p=0.1):\n",
    "        \" Take in model size and number of heads.\"\n",
    "        super().__init__()\n",
    "        assert d_model % h ==0\n",
    "        ## assumption d_v always equals d_k\n",
    "        self.d_k = d_model // h; self.h = h; self.att = None; self.p = nn.Dropout(p=p)\n",
    "        self.linears = clones(nn.Linear(d_model,d_model), 4)\n",
    "    \n",
    "    def forward(self,q,k,v,msk=None):\n",
    "        'Implements figure 2'\n",
    "        if msk is not None:\n",
    "            # same mask for all heads? - Why\n",
    "            msk = msk[:,None,...]\n",
    "        nbtch = q.size(0)\n",
    "        # 1. Do all the linear projections in batch  from d_model => h x d_k\n",
    "        q,k,v = [l(x).view(nbtch,-1,self.h,self.d_k).transpose(1,2)\n",
    "                for l,x in zip(self.linears,(q,k,v))]\n",
    "        # 2. Apply attention to all the projected vectors in batch.\n",
    "        x,self.att = attention(q,k,v,mask=mask,p=self.p)\n",
    "        # 3.'Concat' using a view and apply a final linear.\n",
    "        x = x.transpose(1,2).contigous().view(nbtch,-1,self.h*self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BasicFC(nn.Module):\n",
    "    ''' Base FC to apply at the end of each encoder-decoder block '''\n",
    "    def __init__(self,d_model,d_h,p=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model,d_h)\n",
    "        self.w_2 = nn.Linear(d_h,d_model)\n",
    "        self.p = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.w_2(self.p(F.relu(self.w_1(x))))        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self,d_model,vocab):\n",
    "        super().__init__()\n",
    "        self.lidx = nn.Embedding(d_model,vocab)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.lut(x)*math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,p,max_len=5000):\n",
    "        super().__init__()\n",
    "        self.p = nn.Dropout(p)\n",
    "        # compute the poisitional encodings in log space\n",
    "        pe = torch.zeros(max_len,d_model)\n",
    "        position = torch.arange(0,max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0.,d_model,2)* -(math.log(1000.0)/d_model))\n",
    "#         breakpoint()\n",
    "        pe[:,0::2] = torch.sin(position * div_term).float()\n",
    "        pe[:,1::2] = torch.cos(position * div_term).float()\n",
    "        pe = pe[None]\n",
    "        self.register_buffer('pe',pe)\n",
    "    def forward(self,x):\n",
    "        x  = x + torch.tensor(self.pe[:,x.size(1)])\n",
    "        return self.p(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shang/.conda/envs/pyfast/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "pe = PositionalEncoding(20,0)\n",
    "y = pe.forward(torch.zeros((1,100,20),requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb3378b3400>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(np.arange(100),y[0,:,4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted attention_module.ipynb to nb_attention.py\r\n"
     ]
    }
   ],
   "source": [
    "! ../notebook2script.py ./attention_module.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#################################################\r\n",
      "### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###\r\n",
      "#################################################\r\n",
      "# file to edit: dev_nb/attention_module.ipynb\r\n",
      "\r\n",
      "import os,sys,time,copy,math,numpy as np\r\n",
      "from IPython.core.debugger import set_trace\r\n",
      "import torch,torch.nn as nn,torch.nn.functional as F\r\n",
      "import matplotlib.pyplot as plt\r\n",
      "import seaborn as sns\r\n",
      "\r\n",
      "class EncoderDecoder(nn.Module):\r\n",
      "    '''\r\n",
      "    A standard encoder-decoder architecture...\r\n",
      "    '''\r\n",
      "    def __init__(self,encr,decr,src_emb,tgt_emb,gen):\r\n",
      "        super().__init__()\r\n",
      "        self.encr = encr;self.decr = decr; self.src_emb = src_emb\r\n",
      "        self.tgt_emb = tgt_emb; self.gen = gen\r\n",
      "\r\n",
      "    def forward(self,src,tgt,src_mask,tgt_mask):\r\n",
      "        return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask)\r\n",
      "\r\n",
      "    def encode(self,src,src_mask):\r\n",
      "        return self.encr(self.src_emb(src),src_mask)\r\n",
      "\r\n",
      "    def decode(self,tgt,tgt_mask):\r\n",
      "        return self.decr(self.tgt_emb(tgt),tgt_mask)\r\n",
      "\r\n",
      "\r\n",
      "class Generator(nn.Module):\r\n",
      "    \"\"\"\r\n",
      "    Defines standard linear+ softmax step\r\n",
      "    \"\"\"\r\n",
      "    def __init__(self,d_model,vocab):\r\n",
      "        super().__init__()\r\n",
      "        self.proj = nn.Linear(d_model,vocab)\r\n",
      "\r\n",
      "    def forward(self,x):\r\n",
      "        return F.log_softmax(self.proj(x),dim=-1)\r\n",
      "\r\n",
      "def clones(module,N):\r\n",
      "    ''' Produces N identical modules '''\r\n",
      "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\r\n",
      "\r\n",
      "class Encoder(nn.Module):\r\n",
      "    ''' Core encoder is a stack of N layers '''\r\n",
      "    def __init__(self,layer,N):\r\n",
      "        super().__init__()\r\n",
      "        self.layers = clones(layer,N)\r\n",
      "        self.norm  = LayerNorm(layer.size)\r\n",
      "\r\n",
      "    def forward(self,x,mask):\r\n",
      "        ''' Pass the input and mask through each layer in turn  '''\r\n",
      "        for layer in self.layers:\r\n",
      "            x = layer(x,mask)\r\n",
      "        return self.norm(x)\r\n",
      "\r\n",
      "\r\n",
      "class LayerNorm(nn.Module):\r\n",
      "    def __init__(self,features,eps=1e-6):\r\n",
      "        super().__init__()\r\n",
      "        self.a_2 = nn.Parameter(torch.ones(features))\r\n",
      "        self.b_2 = nn.Parameter(torch.zeros(features))\r\n",
      "        self.eps = eps\r\n",
      "\r\n",
      "    def forward(self,x):\r\n",
      "        mean = x.mean(-1,keepdim=True)\r\n",
      "        std = x.std(-1,keepdim=True)\r\n",
      "        return self.a_2*(x - mean)/(std + self.eps) + self.b_2\r\n",
      "\r\n",
      "class SubLayerConnection(nn.Module):\r\n",
      "    \"\"\" A sum of original + layer(input) followed by layer Norm\"\"\"\r\n",
      "    def __init__(self,size,p):\r\n",
      "        super().__init__()\r\n",
      "        self.norm = LayerNorm(size)\r\n",
      "        self.p = nn.Dropout(p)\r\n",
      "\r\n",
      "    def forward(self,x,sublayer):\r\n",
      "        \"\"\" Apply residual connection to sublayer of same size\"\"\"\r\n",
      "        return x + self.p(sublayer(self.norm(x)))\r\n",
      "\r\n",
      "class BasicEncoderLayer(nn.Module):\r\n",
      "    \"\"\" Basic encoder layer is made of self.attentiona and feed forward layer \"\"\"\r\n",
      "    def __init__(self,sz,slf_att,ff,p):\r\n",
      "        super().__init__()\r\n",
      "        self.slf_att = slf_att; self.sz = sz; self.ff = ff;\r\n",
      "        self.sublayer = clones(SubLayerConnection(size,dropout),2)\r\n",
      "\r\n",
      "    def forward(self,x):\r\n",
      "        x = self.sublayer[0](x,lambda x: self.slf_att(x,x,x,mask))\r\n",
      "        return self.sublayer[1](x,self.ff)\r\n",
      "\r\n",
      "class Decoder(nn.Module):\r\n",
      "    ''' Basic N layer Decoder '''\r\n",
      "    def __init__(self,layer,N):\r\n",
      "        super().__init__()\r\n",
      "        self.layers = clones(layer,N)\r\n",
      "        self.norm = LayerNorm(layer.size)\r\n",
      "\r\n",
      "    def forward(self,x,mem,src_msk,tgt_msk):\r\n",
      "        for layer in self.layers:\r\n",
      "            x = layer(x,mem,src_msk,tgt_msk)\r\n",
      "        return self.norm(x)\r\n",
      "\r\n",
      "class BasicDecoderLayer(nn.Module):\r\n",
      "    ''' Basic N layer Decoder '''\r\n",
      "    def __init__(self,sz,slf_att,src_att,ff,p):\r\n",
      "        super().__init__()\r\n",
      "        self.slf_att = slf_att; self.src_att = src_att; self.ff = ff;\r\n",
      "        self.sublayer  = clones(SubLayerConnection(sz,p), 3)\r\n",
      "\r\n",
      "    def forward(self,x,mem,src_msk,tgt_msk):\r\n",
      "        x = self.sublayer[0](x, lambda x: self.slf_att(x,x,x,tgt_msk))\r\n",
      "        x = self.sublayer[1](x , lambda x: self.src_att(x,x,x,src_msk))\r\n",
      "        return self.sublayer[2](x, lambda x:self.ff(x))\r\n",
      "\r\n",
      "def future_msk(sz):\r\n",
      "    \"\"\" Mask out future positions. \"\"\"\r\n",
      "    att_shp = (1,sz,sz)\r\n",
      "    future_msk = np.triu(np.ones(att_shp),k=1).astype('uint8')\r\n",
      "    return torch.from_numpy(future_msk) ==0\r\n",
      "\r\n",
      "def attention(q,k,v,msk=None,p=None):\r\n",
      "    \"\"\" Compute 'Scaled dot product attention' \"\"\"\r\n",
      "    d_k = q.size(-1)\r\n",
      "    scores = torch.matmul(q,k.transpose(-2,-1))/math.sqrt(d_k)\r\n",
      "    if msk is not None:\r\n",
      "        scores = scores.masked_fill(mask==0,-1e9)\r\n",
      "        p_att = F.softmax(scores,dim = -1)\r\n",
      "        if p is not None:\r\n",
      "            p_att = p(p_att)\r\n",
      "        return torch.matmul(p_att,v), p_att\r\n",
      "\r\n",
      "class MultiHeadAttention(nn.Module):\r\n",
      "    def __init__(self,h, d_model, p=0.1):\r\n",
      "        \" Take in model size and number of heads.\"\r\n",
      "        super().__init__()\r\n",
      "        assert d_model % h ==0\r\n",
      "        ## assumption d_v always equals d_k\r\n",
      "        self.d_k = d_model // h; self.h = h; self.att = None; self.p = nn.Dropout(p=p)\r\n",
      "        self.linears = clones(nn.Linear(d_model,d_model), 4)\r\n",
      "\r\n",
      "    def forward(self,q,k,v,msk=None):\r\n",
      "        'Implements figure 2'\r\n",
      "        if msk is not None:\r\n",
      "            # same mask for all heads? - Why\r\n",
      "            msk = msk[:,None,...]\r\n",
      "        nbtch = q.size(0)\r\n",
      "        # 1. Do all the linear projections in batch  from d_model => h x d_k\r\n",
      "        q,k,v = [l(x).view(nbtch,-1,self.h,self.d_k).transpose(1,2)\r\n",
      "                for l,x in zip(self.linears,(q,k,v))]\r\n",
      "        # 2. Apply attention to all the projected vectors in batch.\r\n",
      "        x,self.att = attention(q,k,v,mask=mask,p=self.p)\r\n",
      "        # 3.'Concat' using a view and apply a final linear.\r\n",
      "        x = x.transpose(1,2).contigous().view(nbtch,-1,self.h*self.d_k)\r\n",
      "        return self.linears[-1](x)\r\n",
      "\r\n",
      "class BasicFC(nn.Module):\r\n",
      "    ''' Base FC to apply at the end of each encoder-decoder block '''\r\n",
      "    def __init__(self,d_model,d_h,p=0.1):\r\n",
      "        super().__init__()\r\n",
      "        self.w_1 = nn.Linear(d_model,d_h)\r\n",
      "        self.w_2 = nn.Linear(d_h,d_model)\r\n",
      "        self.p = nn.Dropout(p)\r\n",
      "\r\n",
      "    def forward(self,x):\r\n",
      "        return self.w_2(self.p(F.relu(self.w_1(x))))\r\n",
      "\r\n",
      "\r\n",
      "class Embeddings(nn.Module):\r\n",
      "    def __init__(self,d_model,vocab):\r\n",
      "        super().__init__()\r\n",
      "        self.lidx = nn.Embedding(d_model,vocab)\r\n",
      "        self.d_model = d_model\r\n",
      "\r\n",
      "    def forward(self,x):\r\n",
      "        return self.lut(x)*math.sqrt(self.d_model)\r\n",
      "\r\n",
      "class PositionalEncoding(nn.Module):\r\n",
      "    def __init__(self,d_model,p,max_len=5000):\r\n",
      "        super().__init__()\r\n",
      "        self.p = nn.Dropout(p)\r\n",
      "        # compute the poisitional encodings in log space\r\n",
      "        pe = torch.zeros(max_len,d_model)\r\n",
      "        position = torch.arange(0,max_len).unsqueeze(1).float()\r\n",
      "        div_term = torch.exp(torch.arange(0.,d_model,2)* -(math.log(1000.0)/d_model))\r\n",
      "#         breakpoint()\r\n",
      "        pe[:,0::2] = torch.sin(position * div_term).float()\r\n",
      "        pe[:,1::2] = torch.cos(position * div_term).float()\r\n",
      "        pe = pe[None]\r\n",
      "        self.register_buffer('pe',pe)\r\n",
      "    def forward(self,x):\r\n",
      "        x  = x + torch.tensor(self.pe[:,x.size(1)])\r\n",
      "        return self.p(x)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./exp/nb_attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyfast]",
   "language": "python",
   "name": "conda-env-pyfast-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
